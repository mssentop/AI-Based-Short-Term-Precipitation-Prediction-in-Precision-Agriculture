{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7240cc0",
   "metadata": {},
   "source": [
    "# calling libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e51ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import tensorflow.keras as keras\n",
    "# from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pywt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b511d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e39bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intervals_without_missing_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['zaman_olcum'] = pd.to_datetime(df['zaman_olcum'], format='%d.%m.%Y %H:%M:%S')\n",
    "\n",
    "    intervals = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        if (df.loc[i, 'zaman_olcum'] - df.loc[i-1, 'zaman_olcum']).seconds / 60 != 10:\n",
    "            intervals.append((df.loc[start_idx, 'zaman_olcum'], df.loc[i-1, 'zaman_olcum'], i - start_idx))\n",
    "            start_idx = i\n",
    "\n",
    "    if start_idx < len(df):\n",
    "        intervals.append((df.loc[start_idx, 'zaman_olcum'], df.loc[len(df)-1, 'zaman_olcum'], len(df) - start_idx))\n",
    "\n",
    "    intervals.sort(key=lambda x: x[2], reverse=True)\n",
    "    return intervals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_number_string(num1, num2):\n",
    "    try:\n",
    "        return float(num1) == float(num2)\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a85ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Define the directory containing the CSV files\n",
    "raw_data_dir = 'StationDataRaw/'\n",
    "\n",
    "def calculate_first_6_hours_arf(rainfall):\n",
    "    \"\"\"\n",
    "    Calculate the Accumulated Rainfall (ARF).\n",
    "    \"\"\"\n",
    "    ARF = 0\n",
    "    for i in range(1, 37):\n",
    "        ARF += rainfall[i]\n",
    " \n",
    "    return ARF\n",
    "\n",
    "def calculate_daily_total_rain(station_data_raw):\n",
    "    station_data_raw['yagis_toplam_mm'] = station_data_raw['yagis_toplam_mm'].str.replace(',', '.')\n",
    "    station_data_raw['yagis_toplam_mm'] = pd.to_numeric(station_data_raw['yagis_toplam_mm'])\n",
    "    # Group by date and calculate the mean of the 'gunes_radyasyon' for each day\n",
    "    daily_total_rain = station_data_raw.groupby('date')['yagis_toplam_mm'].sum().reset_index()\n",
    "\n",
    "    return daily_total_rain\n",
    "\n",
    "# Process each file and calculate the variables for each phenological stage\n",
    "input_data_full = []\n",
    "output_data_full = []\n",
    "count = 0\n",
    "for file_name in os.listdir(raw_data_dir):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    count += 1\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(raw_data_dir, file_name)\n",
    "        longest_interval = find_intervals_without_missing_data(file_path)\n",
    "\n",
    "        if longest_interval[2] < 50000:\n",
    "            continue\n",
    "        # Extract the station code from the file name\n",
    "        station_code = file_name.split('_')[0]\n",
    "        raw_data = pd.read_csv(file_path, usecols=['zaman_olcum', 'sicaklik_2m', 'hava_basinci', 'nispi_nem_2m', 'yagis_toplam_mm'], dtype=str)\n",
    "        # Convert 'zaman_olcum' to datetime\n",
    "        raw_data['zaman_olcum'] = pd.to_datetime(raw_data['zaman_olcum'], format='%d.%m.%Y %H:%M:%S')\n",
    "\n",
    "        filtered_raw_data = raw_data[(raw_data['zaman_olcum'] >= longest_interval[0]) & (raw_data['zaman_olcum'] <= longest_interval[1])]\n",
    "        filtered_raw_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        time = filtered_raw_data['zaman_olcum']\n",
    "        time.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        rf = filtered_raw_data['yagis_toplam_mm'].str.replace(',', '.')\n",
    "        rf = pd.to_numeric(rf)\n",
    "        rf.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        rh = filtered_raw_data['nispi_nem_2m'].str.replace(',', '.')\n",
    "        rh = pd.to_numeric(rh)\n",
    "        rh.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        temp = filtered_raw_data['sicaklik_2m'].str.replace(',', '.')\n",
    "        temp = pd.to_numeric(temp)\n",
    "        temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        pres = filtered_raw_data['hava_basinci'].str.replace(',', '.')\n",
    "        pres = pd.to_numeric(pres)\n",
    "        pres.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        print(count)\n",
    "        \n",
    "        if len(rf) < 31:\n",
    "            continue\n",
    "\n",
    "        isValid = True\n",
    "        for i in range(24, len(rf)-5):\n",
    "            arf_last_4_hours = []\n",
    "            rf_last_4_data = []\n",
    "            rh_last_4_data = []\n",
    "            temp_last_4_data = []\n",
    "            pres_last_4_data = []\n",
    "            for j in range(4):\n",
    "                if np.isnan(rf[i-4+j]) or np.isnan(rh[i-4+j]) or np.isnan(temp[i-4+j]) or np.isnan(pres[i-4+j]):\n",
    "                    isValid = False\n",
    "                    break\n",
    "                one_hour_sum = 0\n",
    "                for k in range(6):\n",
    "                    if np.isnan(rf[(i-24)+6*j+k]):\n",
    "                        isValid = False\n",
    "                        break\n",
    "                    one_hour_sum += rf[(i-24)+6*j+k]\n",
    "                arf_last_4_hours.append(one_hour_sum)\n",
    "                rf_last_4_data.append(rf[i-4+j])\n",
    "                rh_last_4_data.append(rh[i-4+j])\n",
    "                temp_last_4_data.append(temp[i-4+j])\n",
    "                pres_last_4_data.append(pres[i-4+j])\n",
    "\n",
    "            arf_next_1_hour = 0\n",
    "            for j in range(6):\n",
    "                if np.isnan(rf[i+j]):\n",
    "                    isValid = False\n",
    "                    break\n",
    "                arf_next_1_hour += rf[i+j]\n",
    "\n",
    "            raw_data_input = []\n",
    "            raw_data_input.append(station_code)\n",
    "            raw_data_input.append(time[i-1])\n",
    "            raw_data_input.append(arf_last_4_hours)\n",
    "            raw_data_input.append(rf_last_4_data)\n",
    "            raw_data_input.append(rh_last_4_data)\n",
    "            raw_data_input.append(temp_last_4_data)\n",
    "            raw_data_input.append(pres_last_4_data)\n",
    "\n",
    "            raw_data_output = []\n",
    "            raw_data_output.append(station_code)\n",
    "            raw_data_output.append(time[i-1])\n",
    "            raw_data_output.append(arf_next_1_hour)\n",
    "\n",
    "            input_data.append(raw_data_input)\n",
    "            output_data.append(raw_data_output)\n",
    "\n",
    "        if isValid:\n",
    "            input_data_full += input_data\n",
    "            output_data_full += output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfbad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_full = pd.DataFrame(input_data_full).rename({0: 'Station', 1: 'Date', 2: 'ARF', 3: 'RF', 4: 'RH', 5: 'T', 6: 'P'}, axis='columns')\n",
    "output_data_full = pd.DataFrame(output_data_full).rename({0: 'Station', 1: 'Date', 2: 'ARF'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(input_data_full).to_csv('input_dataset.csv', index=False)\n",
    "pd.DataFrame(output_data_full).to_csv('output_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data =  pd.read_csv('input_dataset.csv')\n",
    "output_data = pd.read_csv('output_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = output_data\n",
    "\n",
    "# Count the entries for each station where the 'ARF' column value is greater than 1e-5\n",
    "station_arf_counts = dataset[dataset['ARF'] > 1e-5][dataset.columns[0]].value_counts()\n",
    "\n",
    "# Convert the pandas Series to a list of tuples (station code, number of data entries)\n",
    "station_arf_counts_list = list(station_arf_counts.items())\n",
    "\n",
    "station_arf_counts_list = [elm for elm in station_arf_counts_list if elm[1] > 1500]\n",
    "\n",
    "# Display the result\n",
    "print(\"Station ARF counts as list of tuples:\", station_arf_counts_list)\n",
    "most_rain_recorded_stations = [pair[0] for pair in station_arf_counts_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b48ef0d",
   "metadata": {},
   "source": [
    "# main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for getting the same results.\n",
    "def reset_random_seeds(seed):\n",
    "   os.environ['PYTHONHASHSEED']=str(seed)\n",
    "   tf.random.set_seed(seed)\n",
    "   np.random.seed(seed)\n",
    "   random.seed(seed)\n",
    "reset_random_seeds(42)\n",
    "\n",
    "def successive(successive):\n",
    "    input_data=[]\n",
    "    for i in range(len(successive)-3):\n",
    "        input_data.append([successive[i]]+[successive[i+1]]+[successive[i+2]]+[successive[i+3]])\n",
    "        \n",
    "    #return successive\n",
    "    return input_data  \n",
    "\n",
    "def preprocess_data(input_data, output_data):\n",
    "    first_input_data = list(input_data['RF'])\n",
    "    second_input_data = list(input_data['ARF'])\n",
    "    third_input_data = list(input_data['RH'])\n",
    "    fourth_input_data = list(input_data['T'])\n",
    "    fifth_input_data = list(input_data['P'])\n",
    "    output_data = list(output_data['ARF'])\n",
    "\n",
    "    first_input_successive = first_input_data\n",
    "    second_input_successive = second_input_data\n",
    "    third_input_successive = third_input_data\n",
    "    fourth_input_successive = fourth_input_data\n",
    "    fifth_input_successive = fifth_input_data\n",
    "    output_successive = output_data\n",
    "\n",
    "    #division of data set into training and test data set\n",
    "    N = len(first_input_data)\n",
    "    division_of_training = 0.9\n",
    "    first_input_train = first_input_successive[:int(N*division_of_training)]\n",
    "    first_input_test = first_input_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "    second_input_train = second_input_successive[:int(N*division_of_training)]\n",
    "    second_input_test = second_input_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "    third_input_train = third_input_successive[:int(N*division_of_training)]\n",
    "    third_input_test = third_input_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "    fourth_input_train = fourth_input_successive[:int(N*division_of_training)]\n",
    "    fourth_input_test = fourth_input_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "    fifth_input_train = fifth_input_successive[:int(N*division_of_training)]\n",
    "    fifth_input_test = fifth_input_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "    output_train = output_successive[:int(N*division_of_training)]\n",
    "    output_test = output_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "    return first_input_train, first_input_test, second_input_train, second_input_test, third_input_train, third_input_test, fourth_input_train, fourth_input_test, fifth_input_train, fifth_input_test, output_train, output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0be210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network configurations\n",
    "hidden1=32\n",
    "second_layer1=32\n",
    "third_layer1=32\n",
    "forth_layer1=16\n",
    "\n",
    "hidden2=32\n",
    "second_layer2=16\n",
    "third_layer2=16\n",
    "forth_layer2=8\n",
    "\n",
    "hidden3=32\n",
    "second_layer3=16\n",
    "third_layer3=16\n",
    "forth_layer3=8\n",
    "\n",
    "hidden4=32\n",
    "second_layer4=16\n",
    "third_layer4=16\n",
    "forth_layer4=8\n",
    "\n",
    "hidden5=32\n",
    "second_layer5=16\n",
    "third_layer5=16\n",
    "forth_layer5=8\n",
    "\n",
    "hidden6=32\n",
    "second_layer6=16\n",
    "third_layer6=16\n",
    "forth_layer6=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PECNET2(first_input_train, first_input_test, second_input_train, second_input_test, third_input_train, third_input_test, fourth_input_train, fourth_input_test, fifth_input_train, fifth_input_test, output_train, output_test):\n",
    "    \n",
    "    def standardize_and_normalize(data, epsilon=1e-8):\n",
    "        mean = np.mean(data, axis=1, keepdims=True)\n",
    "        std = np.std(data, axis=1, keepdims=True)\n",
    "        standardized_data = (data - mean) / (std + epsilon)\n",
    "        return standardized_data\n",
    "\n",
    "    # Standardize and normalize the data\n",
    "    first_input_train = standardize_and_normalize(first_input_train)\n",
    "    first_input_test = standardize_and_normalize(first_input_test)\n",
    "    \n",
    "    second_input_train = standardize_and_normalize(second_input_train)\n",
    "    second_input_test = standardize_and_normalize(second_input_test)\n",
    "    \n",
    "    third_input_train = standardize_and_normalize(third_input_train)\n",
    "    third_input_test = standardize_and_normalize(third_input_test)\n",
    "    \n",
    "    fourth_input_train = standardize_and_normalize(fourth_input_train)\n",
    "    fourth_input_test = standardize_and_normalize(fourth_input_test)\n",
    "\n",
    "    fifth_input_train = standardize_and_normalize(fifth_input_train)\n",
    "    fifth_input_test = standardize_and_normalize(fifth_input_test)\n",
    "    \n",
    "    X_train=np.array(first_input_train)\n",
    "    y_train=np.array(output_train)\n",
    "\n",
    "    X_test=np.array(first_input_test)\n",
    "    y_test=np.array(output_test)\n",
    "\n",
    "    m_primary=len(X_train[0,:])\n",
    "    p_primary=np.size(y_train[0])\n",
    "    N_primary=len(X_train)\n",
    "\n",
    "    model= Sequential ([\n",
    "        Dense(hidden1, input_dim=m_primary, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(second_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(third_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(forth_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(p_primary)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "    # Define a learning rate scheduler\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 100:\n",
    "            return lr\n",
    "        elif epoch < 200:\n",
    "            return lr * 0.5\n",
    "        else:\n",
    "            return lr * 0.2\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.1,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    rmsprop = keras.optimizers.legacy.RMSprop(lr=0.1,momentum=0.75, decay=0.0)\n",
    "    adam = keras.optimizers.legacy.Adam(learning_rate=0.0005)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse','mae','mape'])\n",
    "\n",
    "    # Training the model with learning rate scheduler and early stopping\n",
    "    history1=model.fit(X_train, y_train, batch_size=128, epochs=300, shuffle=False, verbose=0, callbacks=[lr_scheduler, early_stopping])  \n",
    "\n",
    "    predicted_train = model.predict(X_train) \n",
    "    predicted_train = np.reshape(predicted_train, (predicted_train.size,))\n",
    "    error_train1=predicted_train-y_train\n",
    "\n",
    "    predicted_test = model.predict(X_test) \n",
    "    predicted_test = np.reshape(predicted_test, (predicted_test.size,))\n",
    "    error_test1=predicted_test-y_test\n",
    "    \n",
    "    ######## Second NN\n",
    "\n",
    "    error_train1a=pd.DataFrame(error_train1)\n",
    "    add_train1=second_input_train\n",
    "    \n",
    "    X_error_train1=np.array(add_train1)\n",
    "    y_error_train1=np.array(error_train1a)\n",
    "\n",
    "    error_test1a=pd.DataFrame(error_test1)\n",
    "    add_test1=second_input_test\n",
    "\n",
    "    X_error_test1=np.array(add_test1)\n",
    "\n",
    "    m_second=len(X_error_train1[0,:])\n",
    "    p_second=np.size(y_train[0])\n",
    "    N_second=len(X_error_train1)\n",
    "\n",
    "    error_model1= Sequential ([\n",
    "        Dense(hidden2, input_dim=m_second, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(second_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(third_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(forth_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(p_second)\n",
    "    ])\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model1.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005), metrics=['mse','mae','mape'])\n",
    "    history2=error_model1.fit(X_error_train1, y_error_train1, batch_size=256, epochs=300, shuffle=False, verbose=0, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    error_predicted_tr1 = error_model1.predict(X_error_train1)\n",
    "    error_predicted_tr1 = np.reshape(error_predicted_tr1, (error_predicted_tr1.size,))\n",
    "    error_predicted_tes1 = error_model1.predict(X_error_test1)\n",
    "    error_predicted_tes1 = np.reshape(error_predicted_tes1, (error_predicted_tes1.size,))\n",
    "\n",
    "    error_train2=(error_train1-error_predicted_tr1)\n",
    "    error_test2=(error_test1-error_predicted_tes1)\n",
    "\n",
    "    ########## Third NN\n",
    "\n",
    "    error_train2a=pd.DataFrame(error_train2)\n",
    "    add_train2=third_input_train \n",
    "    \n",
    "    X_error_train2=np.array(add_train2)\n",
    "    y_error_train2=np.array(error_train2a)\n",
    "\n",
    "    error_test2a=pd.DataFrame(error_test2)\n",
    "    add_test2=third_input_test\n",
    "\n",
    "    X_error_test2=np.array(add_test2)\n",
    "\n",
    "    m_third=len(X_error_train2[0,:])\n",
    "    p_third=np.size(y_error_train2[0])\n",
    "    N_third=len(X_error_train2)\n",
    "\n",
    "    error_model2= Sequential ([\n",
    "        Dense(hidden3, input_dim=m_third, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(second_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(third_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(forth_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(p_third)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.1,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model2.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.00005), metrics=['mse','mae','mape'])\n",
    "    history3=error_model2.fit(X_error_train2, y_error_train2, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[early_stopping])  \n",
    "\n",
    "    error_predicted_tr2 = error_model2.predict(X_error_train2)\n",
    "    error_predicted_tr2 = np.reshape(error_predicted_tr2, (error_predicted_tr2.size,))\n",
    "    error_predicted_tes2 = error_model2.predict(X_error_test2)\n",
    "    error_predicted_tes2 = np.reshape(error_predicted_tes2, (error_predicted_tes2.size,))\n",
    "\n",
    "    error_train3 = (error_train2-error_predicted_tr2)\n",
    "    error_test3 = (error_test2-error_predicted_tes2)\n",
    "\n",
    "    ########## Fourth NN\n",
    "\n",
    "    error_train3a=pd.DataFrame(error_train3)\n",
    "    add_train3=fourth_input_train\n",
    "    \n",
    "    X_error_train3=np.array(add_train3)\n",
    "    y_error_train3=np.array(error_train3a)\n",
    "\n",
    "    error_test3a=pd.DataFrame(error_test3)\n",
    "    add_test3=fourth_input_test\n",
    "\n",
    "    X_error_test3=np.array(add_test3)\n",
    "\n",
    "    m_fourth=len(X_error_train3[0,:])\n",
    "    p_fourth=np.size(y_error_train3[0])\n",
    "    N_fourth=len(X_error_train3)\n",
    "\n",
    "    error_model3= Sequential ([\n",
    "        Dense(hidden4, input_dim=m_fourth, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(second_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(third_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(forth_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(p_fourth)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.1,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model3.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.00005), metrics=['mse','mae','mape'])\n",
    "    history4=error_model3.fit(X_error_train3, y_error_train3, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[early_stopping])  \n",
    "\n",
    "    error_predicted_tr3 = error_model3.predict(X_error_train3)\n",
    "    error_predicted_tr3 = np.reshape(error_predicted_tr3, (error_predicted_tr3.size,))\n",
    "    error_predicted_tes3 = error_model3.predict(X_error_test3)\n",
    "    error_predicted_tes3 = np.reshape(error_predicted_tes3, (error_predicted_tes3.size,))\n",
    "\n",
    "    error_train4 = (error_train3-error_predicted_tr3)\n",
    "    error_test4 = (error_test3-error_predicted_tes3)\n",
    "\n",
    "    ########## Fifth NN\n",
    "\n",
    "    error_train4a=pd.DataFrame(error_train4)\n",
    "    add_train4=fifth_input_train\n",
    "    \n",
    "    X_error_train4=np.array(add_train4)\n",
    "    y_error_train4=np.array(error_train4a)\n",
    "\n",
    "    error_test4a=pd.DataFrame(error_test4)\n",
    "    add_test4=fifth_input_test\n",
    "\n",
    "    X_error_test4=np.array(add_test4)\n",
    "\n",
    "    m_fifth=len(X_error_train4[0,:])\n",
    "    p_fifth=np.size(y_error_train4[0])\n",
    "    N_fifth=len(X_error_train4)\n",
    "\n",
    "    error_model4= Sequential ([\n",
    "        Dense(hidden5, input_dim=m_fifth, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(second_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(third_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(forth_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(p_fifth)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.1,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model4.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.00005), metrics=['mse','mae','mape'])\n",
    "    history5=error_model4.fit(X_error_train4, y_error_train4, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[early_stopping])  \n",
    "\n",
    "    error_predicted_tr4 = error_model4.predict(X_error_train4)\n",
    "    error_predicted_tr4 = np.reshape(error_predicted_tr4, (error_predicted_tr4.size,))\n",
    "    error_predicted_tes4 = error_model4.predict(X_error_test4)\n",
    "    error_predicted_tes4 = np.reshape(error_predicted_tes4, (error_predicted_tes4.size,))\n",
    "\n",
    "    error_train5 = (error_train4-error_predicted_tr4)\n",
    "    error_test5 = (error_test4-error_predicted_tes4)\n",
    "\n",
    "    ##### Sixth NN (Error Network)\n",
    "\n",
    "    error_train5a=pd.DataFrame(error_train5)\n",
    "    for i in range(INPUT_SIZE):\n",
    "        error_train5a[i+1]= error_train5a[i].shift(1)\n",
    "    error_train5a = error_train5a.replace(np.nan, 0)\n",
    "    error_train5a = error_train5a.iloc[:,::-1]\n",
    "    ##error normalization\n",
    "    subtraction_error_train5=np.mean(np.array(error_train5a)[:,:-1], axis=1)\n",
    "    error_train5a -= subtraction_error_train5[:, None]\n",
    "    error_train5a = np.array(error_train5a)\n",
    "    days_train = error_train5a[:,:INPUT_SIZE]\n",
    "    input5_train=days_train\n",
    "    output5_train=error_train5a[:,INPUT_SIZE]\n",
    "\n",
    "    X_error_train5=np.array(input5_train)\n",
    "    y_error_train5=np.array(output5_train)\n",
    "\n",
    "    error_test5a=pd.DataFrame(error_test5)\n",
    "    for i in range(INPUT_SIZE):\n",
    "        error_test5a[i+1]= error_test5a[i].shift(1)\n",
    "    error_test5a = error_test5a.replace(np.nan, 0)\n",
    "    error_test5a = error_test5a.iloc[:,::-1]\n",
    "\n",
    "    subtraction_error_test5=np.array(error_test5a)\n",
    "    subtraction_error_test5=subtraction_error_test5[:,:-1]\n",
    "    subtraction_error_test5=np.mean(subtraction_error_test5, axis=1)\n",
    "    error_test5a -= subtraction_error_test5[:, None]\n",
    "    error_test5a=np.array(error_test5a)\n",
    "    days_test = error_test5a[:,:INPUT_SIZE]\n",
    "    input5_test=days_test\n",
    "    output5_test=error_test5a[:,INPUT_SIZE]\n",
    "\n",
    "    X_error_test5=np.array(input5_test)\n",
    "    \n",
    "    m_error=len(X_error_train5[0,:])\n",
    "    p_error=np.size(y_error_train5[0])\n",
    "    N_error=len(X_error_train5)\n",
    "\n",
    "    error_model5= Sequential ([\n",
    "        Dense(hidden5, input_dim=m_error, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(second_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(third_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(forth_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(p_error)\n",
    "    ])\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model5.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005), metrics=['mse','mae','mape'])\n",
    "    history6=error_model5.fit(X_error_train5, y_error_train5, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    error_predicted_tr5 = error_model5.predict(X_error_train5)\n",
    "    error_predicted_tr5 = np.reshape(error_predicted_tr5, (error_predicted_tr5.size,))\n",
    "    error_predicted_tes5 = error_model5.predict(X_error_test5)\n",
    "    error_predicted_tes5= np.reshape(error_predicted_tes5, (error_predicted_tes5.size,))\n",
    "\n",
    "    compensated_y_train=error_train5-(error_predicted_tr5+subtraction_error_train5)\n",
    "    compensated_y_test=error_test5-(error_predicted_tes5+subtraction_error_test5)\n",
    "\n",
    "    error_predicted_tr6=error_predicted_tr5+subtraction_error_train5\n",
    "    error_predicted_tes6=error_predicted_tes5+subtraction_error_test5\n",
    "\n",
    "    training_final_add=np.column_stack((predicted_train, error_predicted_tr1))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr2))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr3))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr4))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr6))\n",
    "\n",
    "    test_final_add=np.column_stack((predicted_test, error_predicted_tes1))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes2))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes3))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes4))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes6))\n",
    "\n",
    "    ####final NN\n",
    "    m_final=len(training_final_add[0,:])\n",
    "    p_final=np.size(y_train[0])\n",
    "    N_final=len(training_final_add)\n",
    "\n",
    "    final_model= Sequential ([\n",
    "        Dense(hidden6, input_dim=m_final, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(second_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(third_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(forth_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(p_final)\n",
    "    ])\n",
    "\n",
    "    #final_model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    final_model.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['mse','mae','mape'])\n",
    "    final_history=final_model.fit(training_final_add, y_train, batch_size=256, epochs=300, shuffle=False, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    final_predicted_tr = final_model.predict(training_final_add)\n",
    "    final_predicted_tr = np.reshape(final_predicted_tr, (final_predicted_tr.size,))\n",
    "    final_predicted_tes = final_model.predict(test_final_add)\n",
    "    final_predicted_tes = np.reshape(final_predicted_tes, (final_predicted_tes.size,))\n",
    "    \n",
    "    return final_predicted_tes, history1, history2, history3, history4, history5, history6, final_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb2a75b8",
   "metadata": {},
   "source": [
    "## PECNET function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642641b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PECNET(first_input_train, first_input_test, second_input_train, second_input_test, third_input_train, third_input_test, fourth_input_train, fourth_input_test, fifth_input_train, fifth_input_test, output_train, output_test):\n",
    "    \n",
    "    def standardize_and_normalize(data, epsilon=1e-8):\n",
    "        mean = np.mean(data, axis=1, keepdims=True)\n",
    "        std = np.std(data, axis=1, keepdims=True)\n",
    "        standardized_data = (data - mean) / (std + epsilon)\n",
    "        return standardized_data\n",
    "\n",
    "    # Standardize and normalize the data\n",
    "    first_input_train = standardize_and_normalize(first_input_train)\n",
    "    first_input_test = standardize_and_normalize(first_input_test)\n",
    "    \n",
    "    second_input_train = standardize_and_normalize(second_input_train)\n",
    "    second_input_test = standardize_and_normalize(second_input_test)\n",
    "    \n",
    "    third_input_train = standardize_and_normalize(third_input_train)\n",
    "    third_input_test = standardize_and_normalize(third_input_test)\n",
    "    \n",
    "    fourth_input_train = standardize_and_normalize(fourth_input_train)\n",
    "    fourth_input_test = standardize_and_normalize(fourth_input_test)\n",
    "\n",
    "    fifth_input_train = standardize_and_normalize(fifth_input_train)\n",
    "    fifth_input_test = standardize_and_normalize(fifth_input_test)\n",
    "    \n",
    "    X_train=np.array(first_input_train)\n",
    "    y_train=np.array(output_train)\n",
    "\n",
    "    X_test=np.array(first_input_test)\n",
    "    y_test=np.array(output_test)\n",
    "\n",
    "    m_primary=len(X_train[0,:])\n",
    "    p_primary=np.size(y_train[0])\n",
    "    N_primary=len(X_train)\n",
    "\n",
    "    model= Sequential ([\n",
    "        Dense(hidden1, input_dim=m_primary, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(second_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(third_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(forth_layer1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.4),  # Increased dropout rate\n",
    "        Dense(p_primary)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "    # Define a learning rate scheduler\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 50:\n",
    "            return lr\n",
    "        elif epoch < 100:\n",
    "            return lr * 0.5\n",
    "        elif epoch < 150:\n",
    "            return lr * 0.5\n",
    "        else:\n",
    "            return lr * 0.2\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.1,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    rmsprop = keras.optimizers.legacy.RMSprop(lr=0.1,momentum=0.75, decay=0.0)\n",
    "    adam = keras.optimizers.legacy.Adam(learning_rate=0.00025)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse','mae','mape'])\n",
    "\n",
    "    # Training the model with learning rate scheduler and early stopping\n",
    "    history1=model.fit(X_train, y_train, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[lr_scheduler, early_stopping])  \n",
    "\n",
    "    predicted_train = model.predict(X_train) \n",
    "    predicted_train = np.reshape(predicted_train, (predicted_train.size,))\n",
    "    error_train1=predicted_train-y_train\n",
    "\n",
    "    predicted_test = model.predict(X_test) \n",
    "    predicted_test = np.reshape(predicted_test, (predicted_test.size,))\n",
    "    error_test1=predicted_test-y_test\n",
    "    \n",
    "    ######## Second NN\n",
    "\n",
    "    error_train1a=pd.DataFrame(error_train1)\n",
    "    add_train1=second_input_train\n",
    "    \n",
    "    X_error_train1=np.array(add_train1)\n",
    "    y_error_train1=np.array(error_train1a)\n",
    "\n",
    "    error_test1a=pd.DataFrame(error_test1)\n",
    "    add_test1=second_input_test\n",
    "\n",
    "    X_error_test1=np.array(add_test1)\n",
    "\n",
    "    m_second=len(X_error_train1[0,:])\n",
    "    p_second=np.size(y_train[0])\n",
    "    N_second=len(X_error_train1)\n",
    "\n",
    "    error_model1= Sequential ([\n",
    "        Dense(hidden2, input_dim=m_second, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.4),\n",
    "        Dense(second_layer1), #,activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(third_layer1), #,activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(forth_layer1), #,activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(p_second)\n",
    "    ])\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model1.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.00025), metrics=['mse','mae','mape'])\n",
    "    history2=error_model1.fit(X_error_train1, y_error_train1, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    error_predicted_tr1 = error_model1.predict(X_error_train1)\n",
    "    error_predicted_tr1 = np.reshape(error_predicted_tr1, (error_predicted_tr1.size,))\n",
    "    error_predicted_tes1 = error_model1.predict(X_error_test1)\n",
    "    error_predicted_tes1 = np.reshape(error_predicted_tes1, (error_predicted_tes1.size,))\n",
    "\n",
    "    error_train2=(error_train1-error_predicted_tr1)\n",
    "    error_test2=(error_test1-error_predicted_tes1)\n",
    "\n",
    "    ########## Third NN\n",
    "\n",
    "    error_train2a=pd.DataFrame(error_train2)\n",
    "    add_train2=third_input_train \n",
    "    \n",
    "    X_error_train2=np.array(add_train2)\n",
    "    y_error_train2=np.array(error_train2a)\n",
    "\n",
    "    error_test2a=pd.DataFrame(error_test2)\n",
    "    add_test2=third_input_test\n",
    "\n",
    "    X_error_test2=np.array(add_test2)\n",
    "\n",
    "    m_third=len(X_error_train2[0,:])\n",
    "    p_third=np.size(y_error_train2[0])\n",
    "    N_third=len(X_error_train2)\n",
    "\n",
    "    error_model2= Sequential ([\n",
    "        Dense(hidden3, input_dim=m_third, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(second_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(third_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(forth_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(p_third)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.1,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model2.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005), metrics=['mse','mae','mape'])\n",
    "    history3=error_model2.fit(X_error_train2, y_error_train2, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[early_stopping])  \n",
    "\n",
    "    error_predicted_tr2 = error_model2.predict(X_error_train2)\n",
    "    error_predicted_tr2 = np.reshape(error_predicted_tr2, (error_predicted_tr2.size,))\n",
    "    error_predicted_tes2 = error_model2.predict(X_error_test2)\n",
    "    error_predicted_tes2 = np.reshape(error_predicted_tes2, (error_predicted_tes2.size,))\n",
    "\n",
    "    error_train3 = (error_train2-error_predicted_tr2)\n",
    "    error_test3 = (error_test2-error_predicted_tes2)\n",
    "\n",
    "    ########## Fourth NN\n",
    "\n",
    "    error_train3a=pd.DataFrame(error_train3)\n",
    "    add_train3=fourth_input_train\n",
    "    \n",
    "    X_error_train3=np.array(add_train3)\n",
    "    y_error_train3=np.array(error_train3a)\n",
    "\n",
    "    error_test3a=pd.DataFrame(error_test3)\n",
    "    add_test3=fourth_input_test\n",
    "\n",
    "    X_error_test3=np.array(add_test3)\n",
    "\n",
    "    m_fourth=len(X_error_train3[0,:])\n",
    "    p_fourth=np.size(y_error_train3[0])\n",
    "    N_fourth=len(X_error_train3)\n",
    "\n",
    "    error_model3= Sequential ([\n",
    "        Dense(hidden4, input_dim=m_fourth, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(second_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(third_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(forth_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(p_fourth)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.1,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model3.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005), metrics=['mse','mae','mape'])\n",
    "    history4=error_model3.fit(X_error_train3, y_error_train3, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[early_stopping])  \n",
    "\n",
    "    error_predicted_tr3 = error_model3.predict(X_error_train3)\n",
    "    error_predicted_tr3 = np.reshape(error_predicted_tr3, (error_predicted_tr3.size,))\n",
    "    error_predicted_tes3 = error_model3.predict(X_error_test3)\n",
    "    error_predicted_tes3 = np.reshape(error_predicted_tes3, (error_predicted_tes3.size,))\n",
    "\n",
    "    error_train4 = (error_train3-error_predicted_tr3)\n",
    "    error_test4 = (error_test3-error_predicted_tes3)\n",
    "\n",
    "    ########## Fifth NN\n",
    "\n",
    "    error_train4a=pd.DataFrame(error_train4)\n",
    "    add_train4=fifth_input_train\n",
    "    \n",
    "    X_error_train4=np.array(add_train4)\n",
    "    y_error_train4=np.array(error_train4a)\n",
    "\n",
    "    error_test4a=pd.DataFrame(error_test4)\n",
    "    add_test4=fifth_input_test\n",
    "\n",
    "    X_error_test4=np.array(add_test4)\n",
    "\n",
    "    m_fifth=len(X_error_train4[0,:])\n",
    "    p_fifth=np.size(y_error_train4[0])\n",
    "    N_fifth=len(X_error_train4)\n",
    "\n",
    "    error_model4= Sequential ([\n",
    "        Dense(hidden5, input_dim=m_fifth, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(second_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(third_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(forth_layer2), #,activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(p_fifth)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.1,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model4.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005), metrics=['mse','mae','mape'])\n",
    "    history5=error_model4.fit(X_error_train4, y_error_train4, batch_size=512, epochs=300, shuffle=False, verbose=0, callbacks=[early_stopping])  \n",
    "\n",
    "    error_predicted_tr4 = error_model4.predict(X_error_train4)\n",
    "    error_predicted_tr4 = np.reshape(error_predicted_tr4, (error_predicted_tr4.size,))\n",
    "    error_predicted_tes4 = error_model4.predict(X_error_test4)\n",
    "    error_predicted_tes4 = np.reshape(error_predicted_tes4, (error_predicted_tes4.size,))\n",
    "\n",
    "    error_train5 = (error_train4-error_predicted_tr4)\n",
    "    error_test5 = (error_test4-error_predicted_tes4)\n",
    "\n",
    "    ##### Sixth NN (Error Network)\n",
    "\n",
    "    error_train5a=pd.DataFrame(error_train5)\n",
    "    for i in range(INPUT_SIZE):\n",
    "        error_train5a[i+1]= error_train5a[i].shift(1)\n",
    "    error_train5a = error_train5a.replace(np.nan, 0)\n",
    "    error_train5a = error_train5a.iloc[:,::-1]\n",
    "    ##error normalization\n",
    "    subtraction_error_train5=np.mean(np.array(error_train5a)[:,:-1], axis=1)\n",
    "    error_train5a -= subtraction_error_train5[:, None]\n",
    "    error_train5a = np.array(error_train5a)\n",
    "    days_train = error_train5a[:,:INPUT_SIZE]\n",
    "    input5_train=days_train\n",
    "    output5_train=error_train5a[:,INPUT_SIZE]\n",
    "\n",
    "    X_error_train5=np.array(input5_train)\n",
    "    y_error_train5=np.array(output5_train)\n",
    "\n",
    "    error_test5a=pd.DataFrame(error_test5)\n",
    "    for i in range(INPUT_SIZE):\n",
    "        error_test5a[i+1]= error_test5a[i].shift(1)\n",
    "    error_test5a = error_test5a.replace(np.nan, 0)\n",
    "    error_test5a = error_test5a.iloc[:,::-1]\n",
    "\n",
    "    subtraction_error_test5=np.array(error_test5a)\n",
    "    subtraction_error_test5=subtraction_error_test5[:,:-1]\n",
    "    subtraction_error_test5=np.mean(subtraction_error_test5, axis=1)\n",
    "    error_test5a -= subtraction_error_test5[:, None]\n",
    "    error_test5a=np.array(error_test5a)\n",
    "    days_test = error_test5a[:,:INPUT_SIZE]\n",
    "    input5_test=days_test\n",
    "    output5_test=error_test5a[:,INPUT_SIZE]\n",
    "\n",
    "    X_error_test5=np.array(input5_test)\n",
    "    \n",
    "    m_error=len(X_error_train5[0,:])\n",
    "    p_error=np.size(y_error_train5[0])\n",
    "    N_error=len(X_error_train5)\n",
    "\n",
    "    error_model5= Sequential ([\n",
    "        Dense(hidden5, input_dim=m_error, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.2),  # Increased dropout rate\n",
    "        Dense(second_layer1),\n",
    "        Dropout(0.2),  # Increased dropout rate\n",
    "        Dense(third_layer1),\n",
    "        Dropout(0.2),  # Increased dropout rate\n",
    "        Dense(forth_layer1),\n",
    "        Dropout(0.2),  # Increased dropout rate\n",
    "        Dense(p_error)\n",
    "    ])\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    error_model5.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005), metrics=['mse','mae','mape'])\n",
    "    history6=error_model5.fit(X_error_train5, y_error_train5, batch_size=512, epochs=500, shuffle=False, verbose=0, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    error_predicted_tr5 = error_model5.predict(X_error_train5)\n",
    "    error_predicted_tr5 = np.reshape(error_predicted_tr5, (error_predicted_tr5.size,))\n",
    "    error_predicted_tes5 = error_model5.predict(X_error_test5)\n",
    "    error_predicted_tes5= np.reshape(error_predicted_tes5, (error_predicted_tes5.size,))\n",
    "\n",
    "    compensated_y_train=error_train5-(error_predicted_tr5+subtraction_error_train5)\n",
    "    compensated_y_test=error_test5-(error_predicted_tes5+subtraction_error_test5)\n",
    "\n",
    "    error_predicted_tr6=error_predicted_tr5+subtraction_error_train5\n",
    "    error_predicted_tes6=error_predicted_tes5+subtraction_error_test5\n",
    "\n",
    "    training_final_add=np.column_stack((predicted_train, error_predicted_tr1))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr2))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr3))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr4))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr6))\n",
    "\n",
    "    test_final_add=np.column_stack((predicted_test, error_predicted_tes1))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes2))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes3))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes4))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes6))\n",
    "\n",
    "    ####final NN\n",
    "    m_final=len(training_final_add[0,:])\n",
    "    p_final=np.size(y_train[0])\n",
    "    N_final=len(training_final_add)\n",
    "\n",
    "    final_model= Sequential ([\n",
    "        Dense(hidden6, input_dim=m_final, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.2),  # Increased dropout rate\n",
    "        Dense(second_layer1),\n",
    "        Dropout(0.2),  # Increased dropout rate\n",
    "        Dense(third_layer1),\n",
    "        Dropout(0.2),  # Increased dropout rate\n",
    "        Dense(forth_layer1),\n",
    "        Dropout(0.2),  # Increased dropout rate\n",
    "        Dense(p_final)\n",
    "    ])\n",
    "\n",
    "    #final_model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    final_model.compile(loss='mean_squared_error', optimizer=keras.optimizers.legacy.Adam(learning_rate=0.0005), metrics=['mse','mae','mape'])\n",
    "    final_history=final_model.fit(training_final_add, y_train, batch_size=512, epochs=500, shuffle=False, verbose=0, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    final_predicted_tr = final_model.predict(training_final_add)\n",
    "    final_predicted_tr = np.reshape(final_predicted_tr, (final_predicted_tr.size,))\n",
    "    final_predicted_tes = final_model.predict(test_final_add)\n",
    "    final_predicted_tes = np.reshape(final_predicted_tes, (final_predicted_tes.size,))\n",
    "    \n",
    "    return final_predicted_tes, y_test, history1, history2, history3, history4, history5, history6, final_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43808a5",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import ast\n",
    "\n",
    "def build_lstm_model(input_data, output_data):\n",
    "    # Ensure 'Date' columns are of datetime type\n",
    "    input_data['Date'] = pd.to_datetime(input_data['Date'])\n",
    "    output_data['Date'] = pd.to_datetime(output_data['Date'])\n",
    "\n",
    "    # Merge datasets on 'Station' and 'Date'\n",
    "    data = pd.merge(input_data, output_data, on=['Station', 'Date'], suffixes=('_input', ''))\n",
    "\n",
    "    def restructure_data(data):\n",
    "        # Stacking the columns to form single columns for each feature\n",
    "        df_list = []\n",
    "        for i in range(4):  # for RF_0, RF_1, RF_2, RF_3\n",
    "            temp_df = data[['Station', 'Date']]\n",
    "            temp_df['RF'] = data[f'RF_{i}']\n",
    "            temp_df['ARF_input'] = data[f'ARF_{i}']\n",
    "            temp_df['RH'] = data[f'RH_{i}']\n",
    "            temp_df['T'] = data[f'T_{i}']\n",
    "            temp_df['P'] = data[f'P_{i}']\n",
    "            temp_df['ARF'] = data['ARF']\n",
    "            df_list.append(temp_df)\n",
    "\n",
    "        # Concatenate all the dataframes\n",
    "        stacked_df = pd.concat(df_list, axis=0)\n",
    "\n",
    "        # Sort the dataframe by 'Station' and 'Date'\n",
    "        stacked_df = stacked_df.sort_values(by=['Station', 'Date']).reset_index(drop=True)\n",
    "\n",
    "        return stacked_df\n",
    "    \n",
    "    # Function to create input-output pairs\n",
    "    def create_input_output_pairs(data, sequence_length=4):\n",
    "        X, y = [], []\n",
    "        feature_columns = [col for col in data.columns if col not in ['Date', 'Station', 'ARF']]\n",
    "        for i in range(0, len(data), 4):\n",
    "            X.append(data.iloc[i:i+sequence_length][feature_columns].values)\n",
    "            y.append(data.iloc[i]['ARF'])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    stacked_data = restructure_data(data)\n",
    "    X, y = create_input_output_pairs(stacked_data)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=False)\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_shape = X_train.shape\n",
    "    X_test_shape = X_test.shape\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train_shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test_shape)\n",
    "\n",
    "    # Build LSTM model\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(Dense(1))\n",
    "    model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    history_lstm = model_lstm.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Predict using the model\n",
    "    y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "    return y_pred_lstm, y_test, history_lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20540b",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "def prophet(input_data_next, output_data_next):\n",
    "    # Convert 'Date' to datetime format\n",
    "    input_data_next['Date'] = pd.to_datetime(input_data_next['Date'])\n",
    "    output_data_next['Date'] = pd.to_datetime(output_data_next['Date'])\n",
    "    output_data_next = output_data_next.rename(columns={'ARF': 'y'})\n",
    "    \n",
    "    # Merge the shifted input data with the output data\n",
    "    prophet_data = pd.merge(input_data_next, output_data_next, on=['Station', 'Date'], suffixes=('_input', ''))\n",
    "    prophet_data = prophet_data.rename(columns={'Date': 'ds'})\n",
    "    prophet_data = prophet_data.dropna().reset_index(drop=True)  # Drop rows with NaN values resulting from the shift\n",
    "\n",
    "    # Initialize and fit Prophet model\n",
    "    prophet_model = Prophet()\n",
    "    \n",
    "    # Add regressors for each additional feature\n",
    "    additional_features = [col for col in input_data_next.columns if col not in ['Date', 'Station']]\n",
    "    for feature in additional_features:\n",
    "        prophet_model.add_regressor(feature)\n",
    "\n",
    "    # Fit the model\n",
    "    prophet_model.fit(prophet_data[['ds', 'y'] + additional_features])\n",
    "    \n",
    "    # Prepare future dataframe for predictions\n",
    "    future_dates = prophet_data[['ds']].tail(len(output_data_next)).reset_index(drop=True)\n",
    "    future_features = input_data_next[additional_features].tail(len(output_data_next)).reset_index(drop=True)\n",
    "    future_dates = future_dates.join(future_features)\n",
    "\n",
    "    # Make predictions\n",
    "    prophet_forecast = prophet_model.predict(future_dates)\n",
    "    prophet_predictions = prophet_forecast['yhat'].values\n",
    "    prophet_output_test = np.array(output_data_next['y'])\n",
    "    length = int(len(prophet_output_test) * 0.9)\n",
    "    return prophet_predictions[length:], prophet_output_test[length:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10facf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pecnet(input_data_next, output_data_next, pecnet_preds, station_id):\n",
    "    pecnet_preds[station_id] = []\n",
    "    first_input_train, first_input_test, second_input_train, second_input_test, third_input_train, third_input_test, fourth_input_train, fourth_input_test, fifth_input_train, fifth_input_test, output_train, output_test = preprocess_data(input_data_next, output_data_next)\n",
    "    # Predict\n",
    "    print(len(output_test))\n",
    "    pecnet_predictions, pecnet_test, history1, history2, history3, history4, history5, history6, final_history = PECNET(first_input_train, first_input_test, second_input_train, second_input_test, third_input_train, third_input_test, fourth_input_train, fourth_input_test, fifth_input_train, fifth_input_test, output_train, output_test)\n",
    "\n",
    "    pecnet_predictions = np.array(pecnet_predictions)\n",
    "    pecnet_test = np.array(pecnet_test)\n",
    "\n",
    "    pecnet_preds[station_id].append(pecnet_predictions)\n",
    "    pecnet_preds[station_id].append(pecnet_test)\n",
    "    pecnet_preds[station_id].append([history1, history2, history3, history4, history5, history6, final_history])\n",
    "    pecnet_preds[station_id].append(mean_absolute_percentage_error(pecnet_test, pecnet_predictions))\n",
    "    pecnet_preds[station_id].append(mean_squared_error(pecnet_test, pecnet_predictions))\n",
    "    pecnet_preds[station_id].append(mean_absolute_error(pecnet_test, pecnet_predictions))\n",
    "\n",
    "    # Filter out target values less than 1e-5\n",
    "    mask1 = pecnet_test >= 1e-5\n",
    "    output_test_filtered = pecnet_test[mask1]\n",
    "    pecnet_predictions_filtered = pecnet_predictions[mask1]\n",
    "\n",
    "    pecnet_preds[station_id].append(mean_absolute_percentage_error(output_test_filtered, pecnet_predictions_filtered))\n",
    "    pecnet_preds[station_id].append(mean_squared_error(output_test_filtered, pecnet_predictions_filtered))\n",
    "    pecnet_preds[station_id].append(mean_absolute_error(output_test_filtered, pecnet_predictions_filtered))\n",
    "\n",
    "    return pecnet_preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe32df0",
   "metadata": {},
   "source": [
    "## Preprocessing and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e258ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple(input_data_next, output_data_next, pecnet_preds, lstm_preds, prophet_preds, station_id):\n",
    "    pecnet_preds[station_id] = []\n",
    "    lstm_preds[station_id] = []\n",
    "    prophet_preds[station_id] = []\n",
    "    first_input_train, first_input_test, second_input_train, second_input_test, third_input_train, third_input_test, fourth_input_train, fourth_input_test, fifth_input_train, fifth_input_test, output_train, output_test = preprocess_data(input_data_next, output_data_next)\n",
    "    \n",
    "    in_data = input_data_next.copy()\n",
    "\n",
    "    df = pd.DataFrame(list(in_data['RF']), index=in_data.index)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.columns = [f\"{'RF'}_{i}\" for i in range(df.shape[1])]\n",
    "    df_temp = pd.DataFrame(list(in_data['ARF']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'ARF'}_{i}\" for i in range(df_temp.shape[1])]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['RH']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'RH'}_{i}\" for i in range(df_temp.shape[1])]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['T']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'T'}_{i}\" for i in range(df_temp.shape[1])]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['P']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'P'}_{i}\" for i in range(df_temp.shape[1])]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['Date']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'Date'}\"]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df_temp, df], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['Station']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'Station'}\"]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df_temp, df], axis=1)\n",
    "\n",
    "    in_data_next = df\n",
    "    in_data_next.reset_index(drop=True, inplace=True)\n",
    "    in_data_next = in_data_next.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Predict\n",
    "    pecnet_predictions, pecnet_test, history1, history2, history3, history4, history5, history6, final_history = PECNET(first_input_train, first_input_test, second_input_train, second_input_test, third_input_train, third_input_test, fourth_input_train, fourth_input_test, fifth_input_train, fifth_input_test, output_train, output_test)\n",
    "    lstm_predictions, lstm_test, history_lstm = build_lstm_model(in_data_next, output_data_next)\n",
    "    prophet_predictions, prophet_test = prophet(in_data_next, output_data_next)\n",
    "\n",
    "    pecnet_predictions = np.array(pecnet_predictions)\n",
    "    lstm_predictions = np.array(lstm_predictions)\n",
    "    prophet_predictions = np.array(prophet_predictions)\n",
    "\n",
    "    pecnet_test = np.array(pecnet_test)\n",
    "    lstm_test = np.array(lstm_test)\n",
    "    prophet_test = np.array(prophet_test)\n",
    "\n",
    "    pecnet_preds[station_id].append(pecnet_predictions)\n",
    "    pecnet_preds[station_id].append(pecnet_test)\n",
    "    pecnet_preds[station_id].append([history1, history2, history3, history4, history5, history6, final_history])\n",
    "    lstm_preds[station_id].append(lstm_predictions)\n",
    "    lstm_preds[station_id].append(lstm_test)\n",
    "    lstm_preds[station_id].append(history_lstm)\n",
    "    prophet_preds[station_id].append(prophet_predictions)\n",
    "    prophet_preds[station_id].append(prophet_test)\n",
    "    prophet_preds[station_id].append(None)\n",
    "    pecnet_preds[station_id].append(mean_absolute_percentage_error(pecnet_test, pecnet_predictions))\n",
    "    pecnet_preds[station_id].append(mean_squared_error(pecnet_test, pecnet_predictions))\n",
    "    pecnet_preds[station_id].append(mean_absolute_error(pecnet_test, pecnet_predictions))\n",
    "    lstm_preds[station_id].append(mean_absolute_percentage_error(lstm_test, lstm_predictions))\n",
    "    lstm_preds[station_id].append(mean_squared_error(lstm_test, lstm_predictions))\n",
    "    lstm_preds[station_id].append(mean_absolute_error(lstm_test, lstm_predictions))\n",
    "    prophet_preds[station_id].append(mean_absolute_percentage_error(prophet_test, prophet_predictions))\n",
    "    prophet_preds[station_id].append(mean_squared_error(prophet_test, prophet_predictions))\n",
    "    prophet_preds[station_id].append(mean_absolute_error(prophet_test, prophet_predictions))\n",
    "\n",
    "    # Filter out target values less than 1e-5\n",
    "    mask1 = pecnet_test >= 1e-5\n",
    "    pecnet_test_filtered = pecnet_test[mask1]\n",
    "    pecnet_predictions_filtered = pecnet_predictions[mask1]\n",
    "    # Filter out target values less than 1e-5\n",
    "    mask2 = lstm_test >= 1e-5\n",
    "    lstm_test_filtered = lstm_test[mask2]\n",
    "    lstm_predictions_filtered = lstm_predictions[mask2]\n",
    "    # Filter out target values less than 1e-5\n",
    "    mask3 = prophet_test >= 1e-5\n",
    "    prophet_test_filtered = prophet_test[mask3]\n",
    "    prophet_predictions_filtered = prophet_predictions[mask3]\n",
    "\n",
    "    pecnet_preds[station_id].append(mean_absolute_percentage_error(pecnet_test_filtered, pecnet_predictions_filtered))\n",
    "    pecnet_preds[station_id].append(mean_squared_error(pecnet_test_filtered, pecnet_predictions_filtered))\n",
    "    pecnet_preds[station_id].append(mean_absolute_error(pecnet_test_filtered, pecnet_predictions_filtered))\n",
    "    lstm_preds[station_id].append(mean_absolute_percentage_error(lstm_test_filtered, lstm_predictions_filtered))\n",
    "    lstm_preds[station_id].append(mean_squared_error(lstm_test_filtered, lstm_predictions_filtered))\n",
    "    lstm_preds[station_id].append(mean_absolute_error(lstm_test_filtered, lstm_predictions_filtered))\n",
    "    prophet_preds[station_id].append(mean_absolute_percentage_error(prophet_test_filtered, prophet_predictions_filtered))\n",
    "    prophet_preds[station_id].append(mean_squared_error(prophet_test_filtered, prophet_predictions_filtered))\n",
    "    prophet_preds[station_id].append(mean_absolute_error(prophet_test_filtered, prophet_predictions_filtered))\n",
    "\n",
    "    return pecnet_preds, lstm_preds, prophet_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds = {}\n",
    "for i in range(len(most_rain_recorded_stations)): \n",
    "    input_data_next = input_data[input_data[input_data.columns[0]] == most_rain_recorded_stations[i]]\n",
    "    output_data_next = output_data[output_data[output_data.columns[0]] == most_rain_recorded_stations[i]]\n",
    "\n",
    "    in_data = input_data_next.copy()\n",
    "\n",
    "    df = pd.DataFrame(list(in_data['RF']), index=in_data.index)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.columns = [f\"{'RF'}_{i}\" for i in range(df.shape[1])]\n",
    "    df_temp = pd.DataFrame(list(in_data['ARF']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'ARF'}_{i}\" for i in range(df_temp.shape[1])]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['RH']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'RH'}_{i}\" for i in range(df_temp.shape[1])]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['T']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'T'}_{i}\" for i in range(df_temp.shape[1])]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['P']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'P'}_{i}\" for i in range(df_temp.shape[1])]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['Date']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'Date'}\"]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df_temp, df], axis=1)\n",
    "    df_temp = pd.DataFrame(list(in_data['Station']), index=in_data.index)\n",
    "    df_temp.columns = [f\"{'Station'}\"]\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df_temp, df], axis=1)\n",
    "\n",
    "    in_data_next = df\n",
    "    in_data_next.reset_index(drop=True, inplace=True)\n",
    "    in_data_next = in_data_next.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    y_pred_lstm, y_test, history_lstm = build_lstm_model(in_data_next, output_data_next)\n",
    "\n",
    "    station_id = most_rain_recorded_stations[i]\n",
    "    lstm_preds[station_id] = []\n",
    "    lstm_preds[station_id].append(y_pred_lstm)\n",
    "    lstm_preds[station_id].append(y_test)\n",
    "    lstm_preds[station_id].append(history_lstm)\n",
    "    lstm_preds[station_id].append(mean_absolute_percentage_error(y_test, y_pred_lstm))\n",
    "    lstm_preds[station_id].append(mean_squared_error(y_test, y_pred_lstm))\n",
    "    lstm_preds[station_id].append(mean_absolute_error(y_test, y_pred_lstm))\n",
    "    mask2 = y_test >= 1e-5\n",
    "    lstm_test_filtered = y_test[mask2]\n",
    "    lstm_predictions_filtered = y_pred_lstm[mask2]\n",
    "    lstm_preds[station_id].append(mean_absolute_percentage_error(lstm_test_filtered, lstm_predictions_filtered))\n",
    "    lstm_preds[station_id].append(mean_squared_error(lstm_test_filtered, lstm_predictions_filtered))\n",
    "    lstm_preds[station_id].append(mean_absolute_error(lstm_test_filtered, lstm_predictions_filtered))\n",
    "    #prophet_predictions, prophet_output_test = prophet(in_data_next, output_data_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rain_recorded_stations = [pair[0] for pair in station_arf_counts_list]\n",
    "#anomalies = [most_rain_recorded_stations[2], most_rain_recorded_stations[11]]\n",
    "#most_rain_recorded_stations = most_rain_recorded_stations[:1]\n",
    "pecnet_preds = {}\n",
    "lstm_preds = {}\n",
    "prophet_preds = {}\n",
    "\n",
    "#most_rain_recorded_stations.pop(1)\n",
    "input_data = input_data.fillna(method='ffill').fillna(method='bfill')\n",
    "output_data = output_data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "for i in range(len(most_rain_recorded_stations)):\n",
    "    input_data_next = input_data[input_data[input_data.columns[0]] == most_rain_recorded_stations[i]]\n",
    "    output_data_next = output_data[output_data[output_data.columns[0]] == most_rain_recorded_stations[i]]\n",
    "    pecnet_preds, lstm_preds, prophet_preds = run_multiple(input_data_next, output_data_next, pecnet_preds, lstm_preds, prophet_preds, most_rain_recorded_stations[i])\n",
    "    #pecnet_preds = run_pecnet(input_data_next, output_data_next, pecnet_preds, most_rain_recorded_stations[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask9 = y_test > 1e-5\n",
    "mean_absolute_percentage_error(y_test[mask9], y_pred_lstm[mask9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rain_recorded_stations = [pair[0] for pair in station_arf_counts_list]\n",
    "#anomalies = [most_rain_recorded_stations[2], most_rain_recorded_stations[11]]\n",
    "#most_rain_recorded_stations = most_rain_recorded_stations[:1]\n",
    "pecnet_preds = {}\n",
    "lstm_preds = {}\n",
    "prophet_preds = {}\n",
    "\n",
    "#most_rain_recorded_stations.pop(1)\n",
    "input_data = input_data.fillna(method='ffill').fillna(method='bfill')\n",
    "output_data = output_data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "for i in range(len(most_rain_recorded_stations)):\n",
    "    input_data_next = input_data[input_data[input_data.columns[0]] == most_rain_recorded_stations[i]]\n",
    "    output_data_next = output_data[output_data[output_data.columns[0]] == most_rain_recorded_stations[i]]\n",
    "    pecnet_preds, lstm_preds, prophet_preds = run_multiple(input_data_next, output_data_next, pecnet_preds, lstm_preds, prophet_preds, most_rain_recorded_stations[i])\n",
    "    #pecnet_preds = run_pecnet(input_data_next, output_data_next, pecnet_preds, most_rain_recorded_stations[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c06b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1, history2, history3, history4, history5, history6, final_history = pecnet_preds['45.02'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax0,ax1),(ax2,ax3),(ax4,ax5)) = plt.subplots(3,2, figsize=(15,15))\n",
    "ax0.plot(history1.history[\"loss\"])\n",
    "ax0.set_title(\"First Network\")\n",
    "\n",
    "ax1.plot(history2.history[\"loss\"])\n",
    "ax1.set_title(\"Second Network\")\n",
    "\n",
    "ax2.plot(history3.history[\"loss\"])\n",
    "ax2.set_title(\"Third Network\")\n",
    "\n",
    "ax3.plot(history4.history[\"loss\"])\n",
    "ax3.set_title(\"Fourth Network\")\n",
    "\n",
    "ax4.plot(history6.history[\"loss\"])\n",
    "ax4.set_title(\"Error Network\")\n",
    "\n",
    "ax5.plot(final_history.history[\"loss\"])\n",
    "ax5.set_title(\"Final Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds_list_updated = [[key] + value[3:] for key, value in lstm_preds.items()]\n",
    "lstm_preds_list_updated = pd.DataFrame(lstm_preds_list_updated).rename({0: 'Station', 1: 'MAPE (with 0s)', 2: 'MSE (with 0s)', 3: 'MAE (with 0s)', 4: 'MAPE (without 0s)', 5: 'MSE (without 0s)', 6: 'MAE (without 0s)'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed25756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PECNET MSE (with 0s)', np.mean(np.sqrt(lstm_preds_list_updated['MSE (with 0s)'])))\n",
    "print('LSTM MSE (with 0s)', np.mean(np.sqrt(lstm_preds_list['MSE (with 0s)'])))\n",
    "print()\n",
    "\n",
    "print('PECNET MAE (with 0s)', np.mean(lstm_preds_list_updated['MAE (with 0s)']))\n",
    "print('LSTM MAE (with 0s)', np.mean(lstm_preds_list['MAE (with 0s)']))\n",
    "print()\n",
    "\n",
    "print('PECNET MAPE (without 0s)', 100*np.mean(lstm_preds_list_updated['MAPE (without 0s)']), '%')\n",
    "print('LSTM MAPE (without 0s)', 100*np.mean(lstm_preds_list['MAPE (without 0s)']), '%')\n",
    "print()\n",
    "\n",
    "print('PECNET MSE (without 0s)', np.mean(np.sqrt(lstm_preds_list_updated['MSE (without 0s)'])))\n",
    "print('LSTM MSE (without 0s)', np.mean(np.sqrt(lstm_preds_list['MSE (without 0s)'])))\n",
    "print()\n",
    "\n",
    "print('PECNET MAE (without 0s)', np.mean(lstm_preds_list_updated['MAE (without 0s)']))\n",
    "print('LSTM MAE (without 0s)', np.mean(lstm_preds_list['MAE (without 0s)']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aec5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pecnet_preds_list = [[key] + value[3:] for key, value in pecnet_preds.items()]\n",
    "lstm_preds_list = [[key] + value[3:] for key, value in lstm_preds.items()]\n",
    "prophet_preds_list = [[key] + value[3:] for key, value in prophet_preds.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "pecnet_preds_list = [[key] + value for key, value in pecnet_preds.items()]\n",
    "lstm_preds_list = [[key] + value for key, value in lstm_preds.items()]\n",
    "prophet_preds_list = [[key] + value for key, value in prophet_preds.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pecnet_preds_list = pd.DataFrame(pecnet_preds_list)\n",
    "lstm_preds_list = pd.DataFrame(lstm_preds_list)\n",
    "prophet_preds_list = pd.DataFrame(prophet_preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pecnet_preds[list(pecnet_preds.keys())[0]][1] >= 1e-5\n",
    "# Plot PECNET predictions vs true values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pecnet_preds[list(pecnet_preds.keys())[0]][1][mask][-100:], label='True Values')\n",
    "plt.plot(pecnet_preds[list(pecnet_preds.keys())[0]][0][mask][-100:], label='PECNET Predictions')\n",
    "plt.plot(lstm_preds[list(lstm_preds.keys())[0]][0][mask][-100:], label='LSTM Predictions')\n",
    "plt.plot(prophet_preds[list(prophet_preds.keys())[0]][0][mask][-100:], label='Prophet Predictions')\n",
    "plt.title('PECNET Predictions vs True Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pecnet_preds_list).to_csv('pecnet_preds_raw.csv', index=False)\n",
    "pd.DataFrame(lstm_preds_list).to_csv('lstm_preds_raw.csv', index=False)\n",
    "pd.DataFrame(prophet_preds_list).to_csv('prophet_preds_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ec204",
   "metadata": {},
   "outputs": [],
   "source": [
    "pecnet_preds_list = pd.read_csv('pecnet_preds_raw.csv')\n",
    "lstm_preds_list = pd.read_csv('lstm_preds_raw.csv')\n",
    "prophet_preds_list = pd.read_csv('prophet_preds_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = []\n",
    "for i in range(len(most_rain_recorded_stations)):\n",
    "    #true_values[most_rain_recorded_stations[i]] = []\n",
    "    input_data_next = input_data[input_data[input_data.columns[0]] == most_rain_recorded_stations[i]]\n",
    "    output_data_next = output_data[output_data[output_data.columns[0]] == most_rain_recorded_stations[i]]\n",
    "    _, _, _, _, _, _, _, _, _, _, _, output_test = preprocess_data(input_data_next, output_data_next)\n",
    "    true_values.append(np.array(output_test))\n",
    "true_values = np.concatenate(true_values)\n",
    "mask_true = true_values > 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1fbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(true_values[mask_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327005ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pecnet_preds_list = pd.DataFrame(pecnet_preds_list).rename({0: 'Station', 1: 'MAPE (with 0s)', 2: 'MSE (with 0s)', 3: 'MAE (with 0s)', 4: 'MAPE (without 0s)', 5: 'MSE (without 0s)', 6: 'MAE (without 0s)'}, axis='columns')\n",
    "lstm_preds_list = pd.DataFrame(lstm_preds_list).rename({0: 'Station', 1: 'MAPE (with 0s)', 2: 'MSE (with 0s)', 3: 'MAE (with 0s)', 4: 'MAPE (without 0s)', 5: 'MSE (without 0s)', 6: 'MAE (without 0s)'}, axis='columns')\n",
    "prophet_preds_list = pd.DataFrame(prophet_preds_list).rename({0: 'Station', 1: 'MAPE (with 0s)', 2: 'MSE (with 0s)', 3: 'MAE (with 0s)', 4: 'MAPE (without 0s)', 5: 'MSE (without 0s)', 6: 'MAE (without 0s)'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a44652",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PECNET MSE (with 0s)', np.mean(np.sqrt(pecnet_preds_list['MSE (with 0s)'])))\n",
    "print('LSTM MSE (with 0s)', np.mean(np.sqrt(lstm_preds_list['MSE (with 0s)'])))\n",
    "print('Prophet MSE (with 0s)', np.mean(np.sqrt(prophet_preds_list['MSE (with 0s)'])))\n",
    "print()\n",
    "\n",
    "print('PECNET MAE (with 0s)', np.mean(pecnet_preds_list['MAE (with 0s)']))\n",
    "print('LSTM MAE (with 0s)', np.mean(lstm_preds_list['MAE (with 0s)']))\n",
    "print('Prophet MAE (with 0s)', np.mean(prophet_preds_list['MAE (with 0s)']))\n",
    "print()\n",
    "\n",
    "print('PECNET MAPE (without 0s)', 100*np.mean(pecnet_preds_list['MAPE (without 0s)']), '%')\n",
    "print('LSTM MAPE (without 0s)', 100*np.mean(lstm_preds_list['MAPE (without 0s)']), '%')\n",
    "print('Prophet MAPE (without 0s)', 100*np.mean(prophet_preds_list['MAPE (without 0s)']), '%')\n",
    "print()\n",
    "\n",
    "print('PECNET MSE (without 0s)', np.mean(np.sqrt(pecnet_preds_list['MSE (without 0s)'])))\n",
    "print('LSTM MSE (without 0s)', np.mean(np.sqrt(lstm_preds_list['MSE (without 0s)'])))\n",
    "print('Prophet MSE (without 0s)', np.mean(np.sqrt(prophet_preds_list['MSE (without 0s)'])))\n",
    "print()\n",
    "\n",
    "print('PECNET MAE (without 0s)', np.mean(pecnet_preds_list['MAE (without 0s)']))\n",
    "print('LSTM MAE (without 0s)', np.mean(lstm_preds_list['MAE (without 0s)']))\n",
    "print('Prophet MAE (without 0s)', np.mean(prophet_preds_list['MAE (without 0s)']))\n",
    "print()\n",
    "\n",
    "print('Standard deviation of data points (with 0s)', np.std(true_values))\n",
    "print('Standard deviation of data points (without 0s)', np.std(true_values[mask_true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lstm_preds['45.02'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pecnet_preds['45.02'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e416f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prophet_preds['45.02'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PECNET predictions vs true values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(output_test_filtered, label='True Values')\n",
    "plt.plot(pecnet_predictions_filtered, label='PECNET Predictions')\n",
    "plt.title('PECNET Predictions vs True Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out target values less than 1e-5\n",
    "mask1 = pecnet_preds[list(pecnet_preds.keys())[0]][1] >= 1e-5\n",
    "output_test_filtered = pecnet_preds[list(pecnet_preds.keys())[0]][1][mask1]\n",
    "pecnet_predictions_filtered = pecnet_preds[list(pecnet_preds.keys())[0]][0][mask1]\n",
    "# Filter out target values less than 1e-5\n",
    "mask2 = lstm_preds[list(pecnet_preds.keys())[0]][1] >= 1e-5\n",
    "lstm_test_filtered = lstm_preds[list(pecnet_preds.keys())[0]][1][mask2]\n",
    "lstm_predictions_filtered = lstm_preds[list(pecnet_preds.keys())[0]][0][mask2]\n",
    "# Filter out target values less than 1e-5\n",
    "mask3 = prophet_preds[list(pecnet_preds.keys())[0]][1] >= 1e-5\n",
    "prophet_output_test_filtered = prophet_preds[list(pecnet_preds.keys())[0]][1][mask3]\n",
    "prophet_predictions_filtered = prophet_preds[list(pecnet_preds.keys())[0]][0][mask3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "pecnet_true_daily = []\n",
    "pecnet_pred_daily = []\n",
    "for i in range(0, len(pecnet_preds[list(pecnet_preds.keys())[0]][1])-6, 6):\n",
    "    pecnet_true_daily.append(pecnet_preds[list(pecnet_preds.keys())[0]][1][i+5])\n",
    "    pecnet_pred_daily.append(pecnet_preds[list(pecnet_preds.keys())[0]][0][i+5])\n",
    "pecnet_true_daily = np.array(pecnet_true_daily)\n",
    "pecnet_pred_daily = np.array(pecnet_pred_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0edb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hourly_preds(pred_data, k):\n",
    "    preds_weekly_mape = {}\n",
    "    for stat in list(pred_data.keys()):\n",
    "        preds_weekly_mape[stat] = []\n",
    "        true_weekly = []\n",
    "        pred_weekly = []\n",
    "        for i in range(k, len(pred_data[stat][1])-6, 6):\n",
    "            true_weekly.append(pred_data[stat][1][i])\n",
    "            pred_weekly.append(pred_data[stat][0][i])\n",
    "        true_weekly = np.array(true_weekly)\n",
    "        pred_weekly = np.array(pred_weekly)\n",
    "        mask4 = true_weekly >= 1e-5\n",
    "        preds_weekly_mape[stat].append(100*mean_absolute_percentage_error(true_weekly[mask4], pred_weekly[mask4]))\n",
    "    return preds_weekly_mape\n",
    "\n",
    "def calculate_daily_preds(pred_data, k):\n",
    "    preds_daily_mape = {}\n",
    "    for stat in list(pred_data.keys()):\n",
    "        preds_daily_mape[stat] = []\n",
    "        true_daily = []\n",
    "        pred_daily = []\n",
    "        for i in range(k, len(pred_data[stat][1])-144, 144):\n",
    "            true_daily_sum = 0\n",
    "            pred_daily_sum = 0\n",
    "            for j in range(24):\n",
    "                true_daily_sum += pred_data[stat][1][i+6*j]\n",
    "                pred_daily_sum += pred_data[stat][0][i+6*j]\n",
    "            true_daily.append(true_daily_sum)\n",
    "            pred_daily.append(pred_daily_sum)\n",
    "        true_daily = np.array(true_daily)\n",
    "        pred_daily = np.array(pred_daily)\n",
    "        mask4 = true_daily >= 1e-5\n",
    "        preds_daily_mape[stat].append(100*mean_absolute_percentage_error(true_daily[mask4], pred_daily[mask4]))\n",
    "    return preds_daily_mape\n",
    "\n",
    "def calculate_weekly_preds(pred_data, k):\n",
    "    preds_weekly_mape = {}\n",
    "    for stat in list(pred_data.keys()):\n",
    "        preds_weekly_mape[stat] = []\n",
    "        true_weekly = []\n",
    "        pred_weekly = []\n",
    "        for i in range(k, len(pred_data[stat][1])-1008, 1008):\n",
    "            true_weekly_sum = 0\n",
    "            pred_weekly_sum = 0\n",
    "            for j in range(168):\n",
    "                true_weekly_sum += pred_data[stat][1][i+6*j]\n",
    "                pred_weekly_sum += pred_data[stat][0][i+6*j]\n",
    "            true_weekly.append(true_weekly_sum)\n",
    "            pred_weekly.append(pred_weekly_sum)\n",
    "        true_weekly = np.array(true_weekly)\n",
    "        pred_weekly = np.array(pred_weekly)\n",
    "        mask4 = true_weekly >= 1e-5\n",
    "        preds_weekly_mape[stat].append(100*mean_absolute_percentage_error(true_weekly[mask4], pred_weekly[mask4]))\n",
    "    return preds_weekly_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pecnet_preds_hourly_mape_list_means = []\n",
    "lstm_preds_hourly_mape_list_means = []\n",
    "prophet_preds_hourly_mape_list_means = []\n",
    "pecnet_preds_daily_mape_list_means = []\n",
    "lstm_preds_daily_mape_list_means = []\n",
    "prophet_preds_daily_mape_list_means = []\n",
    "pecnet_preds_weekly_mape_list_means = []\n",
    "lstm_preds_weekly_mape_list_means = []\n",
    "prophet_preds_weekly_mape_list_means = []\n",
    "\n",
    "for k in range(6):\n",
    "    pecnet_preds_hourly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_hourly_preds(pecnet_preds, k).items()])\n",
    "    lstm_preds_hourly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_hourly_preds(lstm_preds, k).items()])\n",
    "    prophet_preds_hourly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_hourly_preds(prophet_preds, k).items()])\n",
    "\n",
    "    pecnet_preds_daily_mape_list = pd.DataFrame([[key] + value for key, value in calculate_daily_preds(pecnet_preds, k).items()])\n",
    "    lstm_preds_daily_mape_list = pd.DataFrame([[key] + value for key, value in calculate_daily_preds(lstm_preds, k).items()])\n",
    "    prophet_preds_daily_mape_list = pd.DataFrame([[key] + value for key, value in calculate_daily_preds(prophet_preds, k).items()])\n",
    "\n",
    "    pecnet_preds_weekly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_weekly_preds(pecnet_preds, k).items()])\n",
    "    lstm_preds_weekly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_weekly_preds(lstm_preds, k).items()])\n",
    "    prophet_preds_weekly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_weekly_preds(prophet_preds, k).items()])\n",
    "\n",
    "    pecnet_preds_hourly_mape_list_means.append(np.mean(pecnet_preds_hourly_mape_list[1]))\n",
    "    lstm_preds_hourly_mape_list_means.append(np.mean(lstm_preds_hourly_mape_list[1]))\n",
    "    prophet_preds_hourly_mape_list_means.append(np.mean(prophet_preds_hourly_mape_list[1]))\n",
    "    pecnet_preds_daily_mape_list_means.append(np.mean(pecnet_preds_daily_mape_list[1]))\n",
    "    lstm_preds_daily_mape_list_means.append(np.mean(lstm_preds_daily_mape_list[1]))\n",
    "    prophet_preds_daily_mape_list_means.append(np.mean(prophet_preds_daily_mape_list[1]))\n",
    "    pecnet_preds_weekly_mape_list_means.append(np.mean(pecnet_preds_weekly_mape_list[1]))\n",
    "    lstm_preds_weekly_mape_list_means.append(np.mean(lstm_preds_weekly_mape_list[1]))\n",
    "    prophet_preds_weekly_mape_list_means.append(np.mean(prophet_preds_weekly_mape_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c02ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PECNET HOURLY MAPE (without 0s)', np.mean(pecnet_preds_hourly_mape_list_means), '%')\n",
    "print('LSTM HOURLY MAPE (without 0s)', np.mean(lstm_preds_hourly_mape_list_means), '%')\n",
    "print('Prophet HOURLY MAPE (without 0s)', np.mean(prophet_preds_hourly_mape_list_means), '%')\n",
    "print()\n",
    "print('PECNET DAILY MAPE (without 0s)', np.mean(pecnet_preds_daily_mape_list_means), '%')\n",
    "print('LSTM DAILY MAPE (without 0s)', np.mean(lstm_preds_daily_mape_list_means), '%')\n",
    "print('PROPHET DAILY MAPE (without 0s)', np.mean(prophet_preds_daily_mape_list_means), '%')\n",
    "print()\n",
    "print('PECNET WEEKLY MAPE (without 0s)', np.mean(pecnet_preds_weekly_mape_list_means), '%')\n",
    "print('LSTM WEEKLY MAPE (without 0s)', np.mean(lstm_preds_weekly_mape_list_means), '%')\n",
    "print('PROPHET WEEKLY MAPE (without 0s)', np.mean(prophet_preds_weekly_mape_list_means), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "pecnet_preds_hourly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_hourly_preds(pecnet_preds, k).items()])\n",
    "lstm_preds_hourly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_hourly_preds(lstm_preds, k).items()])\n",
    "prophet_preds_hourly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_hourly_preds(prophet_preds, k).items()])\n",
    "\n",
    "pecnet_preds_daily_mape_list = pd.DataFrame([[key] + value for key, value in calculate_daily_preds(pecnet_preds, k).items()])\n",
    "lstm_preds_daily_mape_list = pd.DataFrame([[key] + value for key, value in calculate_daily_preds(lstm_preds, k).items()])\n",
    "prophet_preds_daily_mape_list = pd.DataFrame([[key] + value for key, value in calculate_daily_preds(prophet_preds, k).items()])\n",
    "\n",
    "pecnet_preds_weekly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_weekly_preds(pecnet_preds, k).items()])\n",
    "lstm_preds_weekly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_weekly_preds(lstm_preds, k).items()])\n",
    "prophet_preds_weekly_mape_list = pd.DataFrame([[key] + value for key, value in calculate_weekly_preds(prophet_preds, k).items()])\n",
    "\n",
    "print('PECNET HOURLY MAPE (without 0s)', np.mean(pecnet_preds_hourly_mape_list[1]), '%')\n",
    "print('LSTM HOURLY MAPE (without 0s)', np.mean(lstm_preds_hourly_mape_list[1]), '%')\n",
    "print('Prophet HOURLY MAPE (without 0s)', np.mean(prophet_preds_hourly_mape_list[1]), '%')\n",
    "print()\n",
    "print('PECNET DAILY MAPE (without 0s)', np.mean(pecnet_preds_daily_mape_list[1]), '%')\n",
    "print('LSTM DAILY MAPE (without 0s)', np.mean(lstm_preds_daily_mape_list[1]), '%')\n",
    "print('PROPHET DAILY MAPE (without 0s)', np.mean(prophet_preds_daily_mape_list[1]), '%')\n",
    "print()\n",
    "print('PECNET WEEKLY MAPE (without 0s)', np.mean(pecnet_preds_weekly_mape_list[1]), '%')\n",
    "print('LSTM WEEKLY MAPE (without 0s)', np.mean(lstm_preds_weekly_mape_list[1]), '%')\n",
    "print('PROPHET WEEKLY MAPE (without 0s)', np.mean(prophet_preds_weekly_mape_list[1]), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ba077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "station_count = np.arange(1, len(pecnet_preds_daily_mape_list) + 1)\n",
    "\n",
    "# Combine the data into a single DataFrame for plotting\n",
    "data = {\n",
    "    'Station': station_count,\n",
    "    'PECNET_MAPE_daily': pecnet_preds_daily_mape_list[1],\n",
    "    'LSTM_MAPE_daily': lstm_preds_daily_mape_list[1],\n",
    "    'Prophet_MAPE_daily': prophet_preds_daily_mape_list[1],\n",
    "    'PECNET_MAPE_weekly': pecnet_preds_weekly_mape_list[1],\n",
    "    'LSTM_MAPE_weekly': lstm_preds_weekly_mape_list[1],\n",
    "    'Prophet_MAPE_weekly': prophet_preds_weekly_mape_list[1],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to create boxplots with error bars for a specific metric\n",
    "def plot_boxplot_with_error_bars(ax, metric_name, metric_cols):\n",
    "    data_to_plot = [df[col] for col in metric_cols]\n",
    "    boxprops = dict(facecolor='lightblue', color='blue')\n",
    "    medianprops = dict(color='red', linewidth=2)\n",
    "    meanprops = dict(marker='o', markerfacecolor='green', markeredgecolor='None', markersize=7)\n",
    "    flierprops = dict(marker='o', markerfacecolor='none', markeredgecolor='purple', markersize=7)\n",
    "\n",
    "    ax.boxplot(data_to_plot, labels=['PECNET', 'LSTM', 'Prophet'], boxprops=boxprops, medianprops=medianprops, meanprops=meanprops, flierprops=flierprops, patch_artist=True, showmeans=True, showfliers=False)\n",
    "    ax.set_title(metric_name.split(' ')[0], fontsize=20)\n",
    "    ax.set_ylabel(metric_name, fontsize=18)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    # Creating custom legend\n",
    "    handles = [\n",
    "        mpatches.Patch(color='lightblue', label='25%-75%'),\n",
    "        mlines.Line2D([], [], color='red', label='Median'),\n",
    "        mlines.Line2D([], [], color='green', marker='o', linestyle='None', markersize=7, label='Mean'),\n",
    "        #mlines.Line2D([], [], color='purple', marker='o', markerfacecolor='none', linestyle='None', markersize=7, label='Outliers')\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=handles, loc='upper left', fontsize=10)\n",
    "\n",
    "# Plotting without zeros (two graphs above, one below centered horizontally)\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# First plot\n",
    "ax1 = fig.add_axes([0.1, 0.55, 0.4, 0.4])\n",
    "plot_boxplot_with_error_bars(ax1, 'DAILY MAPE (%)', ['PECNET_MAPE_daily', 'LSTM_MAPE_daily', 'Prophet_MAPE_daily'])\n",
    "\n",
    "# Second plot\n",
    "ax2 = fig.add_axes([0.55, 0.55, 0.4, 0.4])\n",
    "plot_boxplot_with_error_bars(ax2, 'WEEKLY MAPE (%)', ['PECNET_MAPE_weekly', 'LSTM_MAPE_weekly', 'Prophet_MAPE_weekly'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask4 = pecnet_true_daily >= 1e-5\n",
    "pecnet_true_daily_filtered = pecnet_true_daily[mask4]\n",
    "pecnet_pred_daily_filtered = pecnet_pred_daily[mask4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(pecnet_true_daily_filtered, pecnet_pred_daily_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509fbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PECNET predictions vs true values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pecnet_true_daily_filtered, label='True Values')\n",
    "plt.plot(pecnet_pred_daily_filtered, label='PECNET Predictions')\n",
    "plt.title('PECNET Predictions vs True Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e563789",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pecnet_pred_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cd324",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pecnet_preds[list(pecnet_preds.keys())[0]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pecnet_predictions_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944220fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example data, replace with actual data\n",
    "pecnet_preds = pecnet_preds_list.copy().sort_values(by='Station')\n",
    "lstm_preds = lstm_preds_list.copy().sort_values(by='Station')\n",
    "prophet_preds = prophet_preds_list.copy().sort_values(by='Station')\n",
    "station_count = np.arange(1, len(pecnet_preds) + 1)\n",
    "pecnet_preds['MSE (without 0s)'] = np.sqrt(pecnet_preds['MSE (without 0s)'])\n",
    "lstm_preds['MSE (without 0s)'] = np.sqrt(lstm_preds['MSE (without 0s)'])\n",
    "prophet_preds['MSE (without 0s)'] = np.sqrt(prophet_preds['MSE (without 0s)'])\n",
    "pecnet_preds['MSE (with 0s)'] = np.sqrt(pecnet_preds['MSE (with 0s)'])\n",
    "lstm_preds['MSE (with 0s)'] = np.sqrt(lstm_preds['MSE (with 0s)'])\n",
    "prophet_preds['MSE (with 0s)'] = np.sqrt(prophet_preds['MSE (with 0s)'])\n",
    "pecnet_preds['MAPE (without 0s)'] *= 100\n",
    "lstm_preds['MAPE (without 0s)'] *= 100\n",
    "prophet_preds['MAPE (without 0s)'] *= 100\n",
    "\n",
    "# Function to create bar charts for a specific metric without zeros\n",
    "def plot_metric_without_zeros(ax, metric_name, metric_col):\n",
    "    width = 0.25\n",
    "    ax.bar(station_count - width, pecnet_preds[metric_col], width=width, label='PECNET')\n",
    "    ax.bar(station_count, lstm_preds[metric_col], width=width, label='LSTM')\n",
    "    ax.bar(station_count + width, prophet_preds[metric_col], width=width, label='Prophet')\n",
    "    title = ''\n",
    "    if metric_name == 'MAPE (%)':\n",
    "        title = 'MAPE'\n",
    "    elif metric_name == 'RMSE (mm/h)':\n",
    "        title = 'RMSE'\n",
    "    else:\n",
    "        title = 'MAE'\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_xlabel('Station', fontsize=18)\n",
    "    ax.set_ylabel(metric_name, fontsize=18)\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# Function to create bar charts for a specific metric with zeros\n",
    "def plot_metric_with_zeros(ax, metric_name, metric_col):\n",
    "    width = 0.25\n",
    "    ax.bar(station_count - width, pecnet_preds[metric_col], width=width, label='PECNET')\n",
    "    ax.bar(station_count, lstm_preds[metric_col], width=width, label='LSTM')\n",
    "    ax.bar(station_count + width, prophet_preds[metric_col], width=width, label='Prophet')\n",
    "    title = ''\n",
    "    if metric_name == 'MAPE (%)':\n",
    "        title = 'MAPE'\n",
    "    elif metric_name == 'RMSE (mm/h)':\n",
    "        title = 'RMSE'\n",
    "    else:\n",
    "        title = 'MAE'\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_xlabel('Station', fontsize=18)\n",
    "    ax.set_ylabel(metric_name, fontsize=18)\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# Plotting without zeros (two graphs above, one below centered horizontally)\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# First row, first column\n",
    "ax1 = fig.add_axes([0.1, 0.55, 0.4, 0.4])\n",
    "plot_metric_without_zeros(ax1, 'MAPE (%)', 'MAPE (without 0s)')\n",
    "\n",
    "# First row, second column\n",
    "ax2 = fig.add_axes([0.55, 0.55, 0.4, 0.4])\n",
    "plot_metric_without_zeros(ax2, 'RMSE (mm/h)', 'MSE (without 0s)')\n",
    "\n",
    "# Second row, centered\n",
    "ax3 = fig.add_axes([0.325, 0.05, 0.4, 0.4])\n",
    "plot_metric_without_zeros(ax3, 'MAE (mm/h)', 'MAE (without 0s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting with zeros (excluding MAPE with zeros)\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# First plot\n",
    "ax4 = fig.add_axes([0.1, 0.55, 0.4, 0.4])\n",
    "plot_metric_with_zeros(ax4, 'RMSE (mm/h)', 'MSE (with 0s)')\n",
    "\n",
    "# Second plot\n",
    "ax5 = fig.add_axes([0.55, 0.55, 0.4, 0.4])\n",
    "plot_metric_with_zeros(ax5, 'MAE (mm/h)', 'MAE (with 0s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc996a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example data, replace with actual data\n",
    "pecnet_preds = pecnet_preds_list.copy().sort_values(by='Station')\n",
    "lstm_preds = lstm_preds_list.copy().sort_values(by='Station')\n",
    "prophet_preds = prophet_preds_list.copy().sort_values(by='Station')\n",
    "station_count = np.arange(1, len(pecnet_preds) + 1)\n",
    "pecnet_preds['MSE (without 0s)'] = np.sqrt(pecnet_preds['MSE (without 0s)'])\n",
    "lstm_preds['MSE (without 0s)'] = np.sqrt(lstm_preds['MSE (without 0s)'])\n",
    "prophet_preds['MSE (without 0s)'] = np.sqrt(prophet_preds['MSE (without 0s)'])\n",
    "pecnet_preds['MSE (with 0s)'] = np.sqrt(pecnet_preds['MSE (with 0s)'])\n",
    "lstm_preds['MSE (with 0s)'] = np.sqrt(lstm_preds['MSE (with 0s)'])\n",
    "prophet_preds['MSE (with 0s)'] = np.sqrt(prophet_preds['MSE (with 0s)'])\n",
    "pecnet_preds['MAPE (without 0s)'] *= 100\n",
    "lstm_preds['MAPE (without 0s)'] *= 100\n",
    "prophet_preds['MAPE (without 0s)'] *= 100\n",
    "\n",
    "# Combine the data into a single DataFrame for plotting\n",
    "data = {\n",
    "    'Station': station_count,\n",
    "    'PECNET_MSE_wo0s': pecnet_preds['MSE (without 0s)'],\n",
    "    'LSTM_MSE_wo0s': lstm_preds['MSE (without 0s)'],\n",
    "    'Prophet_MSE_wo0s': prophet_preds['MSE (without 0s)'],\n",
    "    'PECNET_MSE_w0s': pecnet_preds['MSE (with 0s)'],\n",
    "    'LSTM_MSE_w0s': lstm_preds['MSE (with 0s)'],\n",
    "    'Prophet_MSE_w0s': prophet_preds['MSE (with 0s)'],\n",
    "    'PECNET_MAPE_wo0s': pecnet_preds['MAPE (without 0s)'],\n",
    "    'LSTM_MAPE_wo0s': lstm_preds['MAPE (without 0s)'],\n",
    "    'Prophet_MAPE_wo0s': prophet_preds['MAPE (without 0s)'],\n",
    "    'PECNET_MAE_wo0s': pecnet_preds['MAE (without 0s)'],\n",
    "    'LSTM_MAE_wo0s': lstm_preds['MAE (without 0s)'],\n",
    "    'Prophet_MAE_wo0s': prophet_preds['MAE (without 0s)'],\n",
    "    'PECNET_MAE_w0s': pecnet_preds['MAE (with 0s)'],\n",
    "    'LSTM_MAE_w0s': lstm_preds['MAE (with 0s)'],\n",
    "    'Prophet_MAE_w0s': prophet_preds['MAE (with 0s)'],\n",
    "    'PECNET_MAPE_weekly': pecnet_preds_weekly_mape_list[1],\n",
    "    'LSTM_MAPE_weekly': lstm_preds_weekly_mape_list[1],\n",
    "    'Prophet_MAPE_weekly': prophet_preds_weekly_mape_list[1],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to create boxplots with error bars for a specific metric\n",
    "def plot_boxplot_with_error_bars(ax, metric_name, metric_cols):\n",
    "    data_to_plot = [df[col] for col in metric_cols]\n",
    "    boxprops = dict(facecolor='lightblue', color='blue')\n",
    "    medianprops = dict(color='red', linewidth=2)\n",
    "    meanprops = dict(marker='o', markerfacecolor='green', markeredgecolor='None', markersize=7)\n",
    "    flierprops = dict(marker='o', markerfacecolor='none', markeredgecolor='purple', markersize=7)\n",
    "\n",
    "    ax.boxplot(data_to_plot, labels=['PECNET', 'LSTM', 'Prophet'], boxprops=boxprops, medianprops=medianprops, meanprops=meanprops, flierprops=flierprops, patch_artist=True, showmeans=True)\n",
    "    ax.set_title(metric_name.split('(')[0], fontsize=20)\n",
    "    ax.set_ylabel(metric_name, fontsize=18)\n",
    "    if metric_name == 'WEEKLY MAPE (%)':\n",
    "        ax.set_ylabel('MAPE (%)', fontsize=18)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    # Creating custom legend\n",
    "    handles = [\n",
    "        mpatches.Patch(color='lightblue', label='25%-75%'),\n",
    "        mlines.Line2D([], [], color='red', label='Median'),\n",
    "        mlines.Line2D([], [], color='green', marker='o', linestyle='None', markersize=7, label='Mean'),\n",
    "        mlines.Line2D([], [], color='purple', marker='o', markerfacecolor='none', linestyle='None', markersize=7, label='Outliers')\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=handles, loc='upper left', fontsize=10)\n",
    "\n",
    "# Plotting without zeros (two graphs above, one below centered horizontally)\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# First row, first column\n",
    "ax1 = fig.add_axes([0.05, 0.55, 0.4, 0.4])\n",
    "plot_boxplot_with_error_bars(ax1, 'MAPE (%)', ['PECNET_MAPE_wo0s', 'LSTM_MAPE_wo0s', 'Prophet_MAPE_wo0s'])\n",
    "\n",
    "# First row, second column\n",
    "ax2 = fig.add_axes([0.55, 0.55, 0.4, 0.4])\n",
    "plot_boxplot_with_error_bars(ax2, 'WEEKLY MAPE (%)', ['PECNET_MAPE_weekly', 'LSTM_MAPE_weekly', 'Prophet_MAPE_weekly'])\n",
    "\n",
    "# Second row, first column\n",
    "ax3 = fig.add_axes([0.05, 0.05, 0.4, 0.4])\n",
    "plot_boxplot_with_error_bars(ax3, 'RMSE (mm/h)', ['PECNET_MSE_wo0s', 'LSTM_MSE_wo0s', 'Prophet_MSE_wo0s'])\n",
    "\n",
    "# Second row, second column\n",
    "ax4 = fig.add_axes([0.55, 0.05, 0.4, 0.4])\n",
    "plot_boxplot_with_error_bars(ax4, 'MAE (mm/h)', ['PECNET_MAE_wo0s', 'LSTM_MAE_wo0s', 'Prophet_MAE_wo0s'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting with zeros (excluding MAPE with zeros)\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# First plot\n",
    "ax5 = fig.add_axes([0.05, 0.55, 0.4, 0.4])\n",
    "plot_boxplot_with_error_bars(ax5, 'RMSE (mm/h)', ['PECNET_MSE_w0s', 'LSTM_MSE_w0s', 'Prophet_MSE_w0s'])\n",
    "\n",
    "# Second plot\n",
    "ax6 = fig.add_axes([0.55, 0.55, 0.4, 0.4])\n",
    "plot_boxplot_with_error_bars(ax6, 'MAE (mm/h)', ['PECNET_MAE_w0s', 'LSTM_MAE_w0s', 'Prophet_MAE_w0s'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ab227",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rain_recorded_stations[2] = '26.10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13512bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rain_recorded_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea27d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_coordinates = []\n",
    "for station in most_rain_recorded_stations:\n",
    "    station_id = str(station).split('.')\n",
    "    if len(station_id[0]) == 1:\n",
    "        station_id[0] = '0'+ station_id[0]\n",
    "    if len(station_id[1]) == 1:\n",
    "        station_id[1] = '0'+ station_id[1]\n",
    "    station_id = station_id[0] + '.' + station_id[1]\n",
    "    raw_station_coordinate = pd.read_csv(os.path.join('StationDataRaw', station_id + '_raw.csv'), usecols=['enlem', 'boylam'], dtype=str)\n",
    "    enlem, boylam = raw_station_coordinate['enlem'][0], raw_station_coordinate['boylam'][0]\n",
    "    station_coordinates.append([enlem, boylam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data from the JSON file\n",
    "file_path = 'tr-cities.json'  # Replace with the actual file path\n",
    "data = gpd.read_file(file_path)\n",
    "\n",
    "# Plot the map\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "data.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "\n",
    "for enlem, boylam in station_coordinates:\n",
    "    # Mark the specified location on the map\n",
    "    target_location = gpd.GeoDataFrame({'geometry': gpd.points_from_xy([enlem], [boylam])})\n",
    "    target_location.plot(ax=ax, color='red', markersize=80)\n",
    "ax.axis('off')\n",
    "ax.legend()\n",
    "# Show the plot\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9631a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Create a map centered around Turkey\n",
    "map_turkey = folium.Map(location=[39.9208, 32.8541], zoom_start=6, tiles='cartodbpositron')\n",
    "\n",
    "# Iterate over each row in the data and add a circle marker for each station\n",
    "for enlem, boylam in station_coordinates:\n",
    "    enlem = float(enlem.replace(',','.'))\n",
    "    boylam = float(boylam.replace(',','.'))\n",
    "    col = 'blue'\n",
    "    \n",
    "    # Add a circle marker to the map\n",
    "    folium.CircleMarker(location=[enlem, boylam], radius=3, fill=True, color=col, fill_color='blue').add_to(map_turkey)\n",
    "\n",
    "# Save the map as an HTML file\n",
    "map_turkey.save('turkey_map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd939e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('station_list.csv')\n",
    "\n",
    "# Create a map centered around Turkey\n",
    "all_stations_marked = folium.Map(location=[39.9208, 32.8541], zoom_start=6, tiles='cartodbpositron')\n",
    "\n",
    "# Iterate over each row in the data and add a circle marker for each station\n",
    "for index, row in data.iterrows():\n",
    "    code = row['code']\n",
    "    name = row['name']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    col = 'blue'\n",
    "    if code == '63.13':\n",
    "        col = 'blue'\n",
    "    elif code == '63.11':\n",
    "        col = 'green'\n",
    "    elif code == '63.12':\n",
    "        col = 'red'\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Add a circle marker to the map\n",
    "    folium.CircleMarker(location=[lat, lon], radius=3, tooltip=f\"{code}: {name}\", fill=True, color=col, fill_color='blue').add_to(all_stations_marked)\n",
    "\n",
    "# Save the map as an HTML file\n",
    "all_stations_marked.save('all_stations_marked.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc55de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_take = input_data[input_data['Station'] == 45.02]\n",
    "output_take = output_data[output_data['Station'] == 45.02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1622d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_take = input_take.drop(['Station', 'Date'], axis=1)\n",
    "output_take = output_take.drop(['Station', 'Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2eba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "input_take = input_take.applymap(lambda x: ast.literal_eval(x)[3] if isinstance(x, str) and x.startswith('[') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f77fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_take['Target'] = list(output_take['ARF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae891870",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = input_take.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f123e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d8951838d5117fcbb66acd20a3271b2de0c7f82d5e92a0ba87e59707f806a06"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pecnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
